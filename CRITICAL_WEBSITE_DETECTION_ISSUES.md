# üö® CRITICAL: Website Detection & Generation Issues

## üìä **Problem Summary**

### **Issue #1: Generating Sites for Businesses That HAVE Websites**
**SEVERITY**: üî¥ **CRITICAL**

**Finding**: 13 generated sites, **12 out of 13** (92%) actually have existing websites when checked on Google Business!

| # | Business Name | Our Data Says | Reality (Google Business) |
|---|---------------|---------------|---------------------------|
| 1 | East End Plumbers | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 2 | Justin The Plumber LLC | `website_url: NULL` | Unknown (US) |
| 3 | SOS 24/7 Plumbing | `website_url: NULL` | Unknown (US) |
| 4 | Plumbing Point, Inc. | `website_url: NULL` | Unknown (US) |
| 5 | Austin's Greatest Plumbing | `website_url: NULL` | Unknown (US) |
| 6 | Near Me Plumbers | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 7 | London Plumber | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 8 | East London Plumbers | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 9 | Urgent Plumbers London | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 10 | Docklands Plumbing | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 11 | Citywide Plumbers | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 12 | Mayfair Plumbers | `website_url: NULL` | **HAS WEBSITE** ‚úÖ |
| 13 | M J B Plumbing | `website_url: NULL` | Unknown (US) |

**Impact**: We're wasting resources generating sites for businesses that don't need them!

---

### **Issue #2: UK vs US Business Mismatch**
**SEVERITY**: üü° **HIGH**

**Finding**: We scraped mostly US businesses, but generated mostly UK sites!

**Database Stats**:
- **Total Businesses**: 457
- **UK Businesses**: 14 (3%)
- **US Businesses**: 419 (92%)

**Generated Sites**:
- **UK Generated**: 9 out of 13 (69%)
- **US Generated**: 4 out of 13 (31%)

**Top Scraped Cities** (US):
1. Los Angeles, CA: 25 businesses
2. New York, NY: 22 businesses
3. Denver, CO: 20 businesses
4. Jacksonville, FL: 19 businesses
5. Houston, TX: 18 businesses
6. Austin, TX: 16 businesses
7. Brooklyn, NY: 15 businesses

**But we generated 9 sites for London, UK!**

---

### **Issue #3: Website URL Not Being Captured**
**SEVERITY**: üî¥ **CRITICAL**

**Finding**: Outscraper provides website data, but it's not being saved properly.

**Evidence**:
- All 13 generated sites have `website_url: NULL`
- All have `website_validation_status: "pending"` (never validated)
- But Google Business profiles show they DO have websites

**Root Causes**:

1. **Outscraper Field Mapping Issue**:
   ```python
   # backend/services/hunter/scraper.py - Line 258
   "website_url": business.get("website") or business.get("site")
   ```
   - Outscraper might use different field names
   - Need to check: `website`, `site`, `url`, `domain`, etc.

2. **Raw Data Not Saved**:
   ```sql
   SELECT COUNT(raw_data) FROM businesses;
   -- Result: 0 out of 457 have raw_data!
   ```
   - `raw_data` field is NULL for ALL businesses
   - We're losing the original Outscraper response
   - Can't debug what fields they actually sent

---

## üîç **Root Cause Analysis**

### **Why UK Businesses?**

Looking at the test generation logic, there's no geographic filter. The issue is likely:

1. **Test Script** (`test_generation.py`):
   - Someone manually ran this with UK business IDs
   - Script has NO validation to check if website exists
   - It only checks `website_validation_status`, which is "pending" for all

2. **Queue Priority**:
   - UK businesses might have been queued first
   - OR they happened to be the first businesses without websites in the DB

### **Why Website URLs Missing?**

Checking the scraper normalization:

```python
# backend/services/hunter/scraper.py
normalized_business = {
    "website_url": business.get("website") or business.get("site"),
    # ...
    "raw_data": business  # ‚Üê This should save the full response
}
```

**Problem**: `raw_data` is being set, but it's NULL in the database!

**Potential causes**:
1. Field is too large for database (unlikely - it's JSONB)
2. Business service is filtering it out
3. Serialization issue

---

## üõ†Ô∏è **Solutions Required**

### **1. IMMEDIATE: Stop Generating for Businesses with Websites**

**Priority**: üî¥ **URGENT**

**Action**: Add enhanced website detection BEFORE generation:

```python
# Enhanced detection workflow:
1. Check website_url field (if not NULL)
2. If NULL, do Google Search API check
3. If still no website found, THEN generate
```

**Implementation**:
- Use Google Search API or SerpAPI
- Search for: `"{business_name}" {city} {state} website`
- Parse results for business website domain
- Update database with found URL
- Mark as "valid" if website found

---

### **2. Fix Website URL Capture from Outscraper**

**Priority**: üî¥ **CRITICAL**

**Steps**:

1. **Debug Outscraper Response**:
   ```python
   # Add logging to scraper.py
   logger.info(f"Raw Outscraper response: {business}")
   logger.info(f"Available keys: {business.keys()}")
   logger.info(f"Website fields: website={business.get('website')}, site={business.get('site')}")
   ```

2. **Check All Possible Website Fields**:
   ```python
   website_url = (
       business.get("website") or 
       business.get("site") or 
       business.get("url") or 
       business.get("domain") or
       business.get("website_url") or
       business.get("business_url")
   )
   ```

3. **Fix raw_data Storage**:
   - Verify why it's NULL
   - Ensure it's being passed to `create_or_update_business`
   - Check database column size/constraints

---

### **3. Implement Google Search Verification**

**Priority**: üü° **HIGH**

**Options**:

#### **Option A: Google Custom Search API** (Recommended)
- **Cost**: Free for 100 searches/day, then $5/1000 searches
- **Accuracy**: High
- **Speed**: Fast (~1-2 seconds)

```python
import requests

def google_search_website(business_name: str, city: str, state: str) -> str:
    """Search Google for business website."""
    API_KEY = settings.GOOGLE_SEARCH_API_KEY
    SEARCH_ENGINE_ID = settings.GOOGLE_SEARCH_ENGINE_ID
    
    query = f'"{business_name}" {city} {state} website'
    url = f"https://www.googleapis.com/customsearch/v1"
    params = {
        "key": API_KEY,
        "cx": SEARCH_ENGINE_ID,
        "q": query,
        "num": 3
    }
    
    response = requests.get(url, params=params)
    results = response.json().get("items", [])
    
    for result in results:
        link = result.get("link")
        # Filter out Google Maps, Yelp, Facebook, etc.
        if is_business_website(link):
            return link
    
    return None
```

#### **Option B: SerpAPI** (Alternative)
- **Cost**: $50/month for 5,000 searches
- **Accuracy**: High
- **Speed**: Fast
- **Bonus**: Structured data extraction

#### **Option C: Playwright Web Scraping** (Free but slower)
- **Cost**: Free
- **Accuracy**: Medium
- **Speed**: Slow (~10-20 seconds)
- Already have Playwright set up for validation

---

### **4. Enhanced Validation Workflow**

**New Multi-Stage Verification**:

```
Stage 1: Outscraper Data
‚îú‚îÄ Has website_url? ‚Üí Validate with Playwright
‚îî‚îÄ No website_url? ‚Üí Go to Stage 2

Stage 2: Google Search API
‚îú‚îÄ Search: "{name}" {city} {state} website
‚îú‚îÄ Found website? ‚Üí Update DB ‚Üí Validate with Playwright
‚îî‚îÄ No website? ‚Üí Go to Stage 3

Stage 3: Manual Google Maps Check
‚îú‚îÄ Use gmb_place_id to check Google Maps API
‚îú‚îÄ Found website? ‚Üí Update DB
‚îî‚îÄ No website? ‚Üí SAFE TO GENERATE ‚úÖ
```

---

## üìã **Implementation Plan**

### **Phase 1: Immediate Fixes** (Today)

1. ‚úÖ **Audit Current Generated Sites**
   - Manually check each site on Google Business
   - Document which ones have existing websites
   - Mark as "should_not_have_generated"

2. ‚úÖ **Add Logging to Scraper**
   - Log all Outscraper fields received
   - Log website detection attempts
   - Save full response to file for analysis

3. ‚úÖ **Fix raw_data Storage**
   - Debug why it's NULL
   - Ensure it's being saved properly

### **Phase 2: Enhanced Detection** (This Week)

1. ‚è≥ **Implement Google Search API Check**
   - Set up Google Custom Search API
   - Add search function
   - Integrate into scraping workflow

2. ‚è≥ **Add Pre-Generation Validation**
   - Before queuing generation, do full website check
   - Update business record with found website
   - Only generate if truly no website exists

3. ‚è≥ **Update Generation Queue Logic**
   - Add safety checks
   - Geographic filtering (only generate for target regions)
   - Quality threshold (min rating, min reviews)

### **Phase 3: Data Quality** (Next Week)

1. ‚è≥ **Re-scrape UK Businesses**
   - With enhanced website detection
   - With proper raw_data storage
   - Verify website URLs are captured

2. ‚è≥ **Re-validate All Businesses**
   - Run Google Search API check on all NULL website_url
   - Update database with found websites
   - Flag businesses truly without websites

3. ‚è≥ **Cleanup Bad Generations**
   - Archive or delete generated sites for businesses with websites
   - Focus on businesses that truly need sites

---

## üéØ **Recommended Next Steps**

### **Immediate Actions**:

1. **Verify Website Existence for All 13 Generated Sites**:
   - Manually check Google Business for each
   - Document actual website URLs
   - Update database

2. **Stop Auto-Generation Until Fixed**:
   - Disable automatic queuing
   - Only generate manually after verification

3. **Implement Google Search API**:
   - Get API credentials
   - Build search function
   - Test on 10 businesses

### **Questions to Answer**:

1. **How did UK businesses get queued?**
   - Check Celery logs for generation tasks
   - Look for test scripts or manual commands
   - Review task scheduling

2. **What fields does Outscraper actually send?**
   - Scrape 1 business with full logging
   - Document all fields returned
   - Verify website field name

3. **Why is raw_data NULL?**
   - Check database constraints
   - Review BusinessService code
   - Test data flow

---

## üí∞ **Cost Estimate for Google Search API**

**Current Database**: 457 businesses
- 130 with website_url (already have)
- 327 need Google Search verification

**Cost Calculation**:
- Free tier: 100 searches/day = 0 businesses
- Paid tier: $5/1000 searches
- 327 searches = $1.64 one-time cost
- Monthly maintenance (new businesses): ~$5-10/month

**ROI**: Prevents generating useless sites = Saves way more than $1.64!

---

## ‚úÖ **Success Criteria**

1. **No False Positives**: Never generate a site for a business that has a website
2. **High Capture Rate**: Find 95%+ of existing websites
3. **Fast Processing**: <5 seconds additional time per business
4. **Cost Effective**: <$10/month for search API

---

## üö® **Risk Assessment**

**If Not Fixed**:
- ‚ùå Waste resources on unnecessary sites
- ‚ùå Damage reputation (offering sites to businesses that don't need them)
- ‚ùå Lower conversion (targeting wrong businesses)
- ‚ùå Poor ROI on site generation costs

**If Fixed**:
- ‚úÖ Target businesses that truly need sites
- ‚úÖ Higher conversion rates
- ‚úÖ Better resource utilization
- ‚úÖ Accurate business intelligence

---

## üìù **Action Items**

- [ ] Manually verify all 13 generated sites on Google Business
- [ ] Set up Google Custom Search API credentials
- [ ] Add comprehensive logging to scraper
- [ ] Fix raw_data storage issue
- [ ] Implement Google Search verification function
- [ ] Update generation queue to include verification
- [ ] Re-validate all businesses without website_url
- [ ] Document Outscraper field mappings

---

**Status**: üî¥ **CRITICAL - IMMEDIATE ACTION REQUIRED**

**Owner**: Development Team
**Priority**: P0 (Highest)
**Est. Time**: 2-3 days for full fix

