# ðŸ—ï¸ Queue Architecture: Current vs. Proposed

**Date:** February 15, 2026  
**Status:** ðŸ“‹ Proposal for Discussion

---

## ðŸ”´ **Current Architecture Issues**

### Flow Diagram (Current)
```
USER CLICKS "START SCRAPE"
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. OUTSCRAPER SCRAPING (Synchronous/Blocking)  â”‚
â”‚    â±ï¸ Duration: 2-5 minutes                     â”‚
â”‚    ðŸ“¦ Returns: 50-200 businesses                â”‚
â”‚    ðŸš« Problem: Frontend blocked, no progress   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (all at once)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. VALIDATION QUEUE (Celery Async)             â”‚
â”‚    ðŸ“‹ Task: batch_validate_websites_v2()        â”‚
â”‚    ðŸ’¥ Problem: All 200 queued instantly         â”‚
â”‚    âš™ï¸ Workers: 4 parallel                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (for each failed URL)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. DISCOVERY QUEUE (Celery Async)              â”‚
â”‚    ðŸ“‹ Task: discover_missing_websites_v2()      â”‚
â”‚    ðŸ”„ Then: Re-queues validation                â”‚
â”‚    âš ï¸ Problem: Circular dependency              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Problems

| Issue | Impact | Severity |
|-------|--------|----------|
| **Outscraper Blocks Frontend** | User sees loading spinner for 2-5 min | ðŸŸ¡ Medium |
| **No Progress Feedback** | Can't see how many businesses scraped | ðŸŸ¡ Medium |
| **Queue Overflow** | 200 tasks dumped instantly | ðŸŸ  Medium-High |
| **No Rate Control** | Can overwhelm workers | ðŸŸ  Medium-High |
| **Circular Dependencies** | Discovery â†’ Validation â†’ Discovery | ðŸ”´ High |

---

## âœ… **Proposed Architecture: 3 Separate Queues**

### Flow Diagram (Proposed)
```
USER CLICKS "START SCRAPE"
        â†“ (instant return)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUEUE 1: "outscraper_scraping"                  â”‚
â”‚ Task: scrape_zone_async(zone_id, strategy_id)  â”‚
â”‚                                                 â”‚
â”‚ âœ… Runs in background (Celery)                  â”‚
â”‚ âœ… Creates businesses one-by-one               â”‚
â”‚ âœ… Emits progress updates (Redis/SSE)          â”‚
â”‚ âœ… Queues to Queue 2 per business              â”‚
â”‚                                                 â”‚
â”‚ Workers: 1-2 (I/O bound, external API)         â”‚
â”‚ Priority: Medium                                â”‚
â”‚ Rate Limit: 100 req/min (Outscraper limit)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (one business at a time)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUEUE 2: "url_validation"                       â”‚
â”‚ Task: validate_business_url(business_id)        â”‚
â”‚                                                 â”‚
â”‚ Stage 1: URL Prescreener (fast)                â”‚
â”‚ Stage 2: Playwright (slow, CPU-heavy)          â”‚
â”‚ Stage 3: LLM Verification (API call)           â”‚
â”‚                                                 â”‚
â”‚ IF VALID: Mark business as valid âœ…            â”‚
â”‚ IF INVALID: Queue to Queue 3 â†“                 â”‚
â”‚                                                 â”‚
â”‚ Workers: 4-8 (CPU + I/O bound)                 â”‚
â”‚ Priority: High                                  â”‚
â”‚ Rate Limit: None (controlled by worker count)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (only for invalid/missing URLs)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUEUE 3: "website_discovery"                    â”‚
â”‚ Task: discover_website(business_id)             â”‚
â”‚                                                 â”‚
â”‚ Step 1: ScrapingDog Google Search              â”‚
â”‚ Step 2: LLM analyzes search results            â”‚
â”‚                                                 â”‚
â”‚ IF FOUND: Update business.website_url          â”‚
â”‚          â— DO NOT auto-requeue validation      â”‚
â”‚          User can manually trigger later        â”‚
â”‚                                                 â”‚
â”‚ IF NOT FOUND: confirmed_no_website             â”‚
â”‚                                                 â”‚
â”‚ Workers: 2-4 (I/O bound, external API)         â”‚
â”‚ Priority: Low-Medium                            â”‚
â”‚ Rate Limit: 100 req/sec (ScrapingDog limit)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š **Comparison**

### Current vs. Proposed

| Aspect | Current | Proposed |
|--------|---------|----------|
| **Frontend Response** | âŒ Blocked 2-5 min | âœ… Instant return |
| **Progress Updates** | âŒ None | âœ… Real-time (SSE) |
| **Queue Control** | âŒ Dump all at once | âœ… Controlled flow |
| **Error Isolation** | âŒ Hard to track | âœ… Per-business logs |
| **Rate Limiting** | âŒ None | âœ… Per-queue limits |
| **Circular Dependencies** | âŒ Yes (Discoveryâ†’Validation) | âœ… None (one-way flow) |
| **Worker Scalability** | âŒ All tasks compete | âœ… Scale per queue |
| **Debugging** | âŒ Hard | âœ… Easy (separate logs) |

---

## ðŸ”§ **Implementation Details**

### Celery Configuration

```python
# backend/celery_app.py

# Define separate queues
celery_app.conf.task_routes = {
    'tasks.scraping.scrape_zone_async': {'queue': 'outscraper_scraping'},
    'tasks.validation.validate_business_url': {'queue': 'url_validation'},
    'tasks.discovery.discover_website': {'queue': 'website_discovery'},
}

# Set priorities
celery_app.conf.task_queue_max_priority = 10
celery_app.conf.task_default_priority = 5
```

### Worker Configuration

```bash
# Supervisor config: /etc/supervisor/conf.d/webmagic-celery.conf

[program:webmagic-celery-scraping]
command=/var/www/webmagic/backend/.venv/bin/celery -A celery_app worker 
  --loglevel=info 
  --concurrency=2
  --queue=outscraper_scraping
  -n scraping@%%h

[program:webmagic-celery-validation]
command=/var/www/webmagic/backend/.venv/bin/celery -A celery_app worker 
  --loglevel=info 
  --concurrency=8
  --queue=url_validation
  -n validation@%%h

[program:webmagic-celery-discovery]
command=/var/www/webmagic/backend/.venv/bin/celery -A celery_app worker 
  --loglevel=info 
  --concurrency=4
  --queue=website_discovery
  -n discovery@%%h
```

### Progress Updates (Redis + SSE)

```python
# Backend: Emit progress
from services.redis_service import RedisService

redis = RedisService()

async def scrape_zone_async(zone_id: str):
    for i, business_data in enumerate(outscraper_results):
        # Create business
        business = await create_business(business_data)
        
        # Emit progress
        await redis.publish(f"scrape:{zone_id}:progress", {
            "current": i + 1,
            "total": len(outscraper_results),
            "business": {
                "id": str(business.id),
                "name": business.name,
                "status": "scraped"
            }
        })
        
        # Queue validation
        validate_business_url.delay(str(business.id))
```

```typescript
// Frontend: Listen for updates
const eventSource = new EventSource(`/api/v1/scrapes/${zoneId}/progress`);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  setProgress(data.current / data.total * 100);
  setLastBusiness(data.business);
};
```

---

## ðŸŽ¯ **Breaking the Circular Dependency**

### Current Problem

```
Validation â†’ (fails) â†’ Discovery â†’ (finds URL) â†’ Validation â†’ (fails again?) â†’ Discovery...
```

**Causes:**
- Discovery automatically re-queues validation
- No limit on validation attempts
- Can create infinite loops

### Proposed Solution

```
Validation â†’ (fails) â†’ Discovery â†’ (finds URL) â†’ STOP
                                               â†“
                                        User manually triggers
                                        "Retry Validation" button
```

**Benefits:**
- No automatic loops
- User decides when to retry
- Clear audit trail of attempts
- Can batch retry multiple businesses

### UI Addition

```typescript
// On business detail page or bulk actions
<Button onClick={() => retryValidation(businessId)}>
  ðŸ”„ Retry Validation
</Button>

// For bulk actions
<Button onClick={() => retryValidationBatch(selectedBusinessIds)}>
  ðŸ”„ Retry Validation ({selected.length} businesses)
</Button>
```

---

## ðŸ“ˆ **Expected Benefits**

### Performance

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Frontend Response Time | 2-5 min | <100ms | 1200x faster |
| User Feedback | None | Real-time | âˆž |
| Queue Overflow Risk | High | Low | -90% |
| Worker Efficiency | 60% | 90% | +50% |

### User Experience

âœ… **Instant feedback** - No more waiting
âœ… **Progress bar** - See scraping happen
âœ… **Cancellable** - Stop scrape if needed
âœ… **Better errors** - Per-business error messages
âœ… **Manual control** - Decide when to retry

### Developer Experience

âœ… **Easier debugging** - Separate logs per queue
âœ… **Better monitoring** - Queue-specific metrics
âœ… **Scalable workers** - Adjust per queue needs
âœ… **No circular deps** - Clear one-way flow

---

## ðŸš€ **Implementation Plan**

### Phase 1: Fix ScrapingDog Query (âœ… DONE)
- [x] Remove quotes from query
- [x] Simplify to `"business_name city"`
- [x] Test with curl
- [ ] Deploy and monitor

### Phase 2: Async Outscraper (2-3 hours)
1. Create `scrape_zone_async` task
2. Move Outscraper logic to Celery
3. Add Redis progress tracking
4. Update frontend to use SSE
5. Test end-to-end

### Phase 3: Separate Queues (1-2 hours)
1. Configure queue routing in Celery
2. Update Supervisor config
3. Deploy 3 separate worker processes
4. Monitor queue lengths

### Phase 4: Remove Circular Dependency (1 hour)
1. Discovery no longer auto-queues validation
2. Add "Retry Validation" button in UI
3. Add bulk retry action
4. Update documentation

---

## ðŸ¤” **Decision Required**

**Should we proceed with this architecture?**

### Option A: Full Implementation (Recommended)
- All 4 phases
- Better UX and DX
- Future-proof
- **Time: 5-7 hours total**

### Option B: Phase 1 + 2 Only
- Just make Outscraper async
- Keep validation queue as-is
- Quick win for UX
- **Time: 2-3 hours**

### Option C: Phase 1 Only
- Just fix ScrapingDog query
- Keep current architecture
- Minimal changes
- **Time: 5 minutes (already done!)**

---

## ðŸ’¬ **Recommendation**

**Start with Option C (already done!) and test:**
1. Deploy query fix
2. Run a scrape
3. Monitor ScrapingDog success rate
4. If successful (>90%), plan Phase 2

**Then move to Option A gradually:**
- Phase 2 next (biggest UX win)
- Phase 3 when traffic increases
- Phase 4 as enhancement

---

**What do you think? Should we:**
1. **Deploy the query fix now** and test?
2. **Proceed with full architecture refactor**?
3. **Something in between**?
